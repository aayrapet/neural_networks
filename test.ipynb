{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38a1acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import MLP_Classifier,Layer\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a00a8",
   "metadata": {},
   "source": [
    "## MLP with multilabel classification with python 3.13.11\n",
    "\n",
    "before diving into images, let's see how our simple neural network will perform on tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67427120",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Generate  dataset with 4 labels\n",
    "X, Y = make_classification(\n",
    "    n_samples=6000,     \n",
    "    n_features=4,       \n",
    "    n_redundant=0,      \n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,         # Add label noise\n",
    "    class_sep=1.0,      # Class separation\n",
    "    n_classes=3,      # nb classes\n",
    ")\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 0.5, X.shape)\n",
    "X = X + noise\n",
    "X=pd.DataFrame(X)\n",
    "Y=pd.Series(Y)\n",
    "if len(np.unique(Y))==2:\n",
    "   Y=pd.DataFrame(Y)\n",
    "else: \n",
    "   Y=pd.get_dummies(Y).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8c3c0",
   "metadata": {},
   "source": [
    "determine objective function to optimise :  minimise cross entropy (in pdf maximise log-likelihood).\n",
    "\n",
    "for example we optimise over batch size, learning rate and dropout (one of the most important parameters in NN).\n",
    "\n",
    "we could do also on layers but computantionally expensive for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15db729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-05 20:45:37,409] A new study created in RDB with name: MLP\n",
      "[I 2026-01-05 20:45:44,848] Trial 0 finished with value: 1.0988307655332203 and parameters: {'batch_size': 529, 'alpha': 0.013827387119549409, 'dropout': 0.7932257008509562}. Best is trial 0 with value: 1.0988307655332203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': 529, 'alpha': 0.013827387119549409, 'dropout': 0.7932257008509562}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 500, 800)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.1)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.5, 0.9)\n",
    "\n",
    "    model = MLP_Classifier(\n",
    "        (\n",
    "            (\n",
    "                Layer(\n",
    "                    nb_neurons=20,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                    batchnorm=True\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=10,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=30,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"dropout\", dropout_rate),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "        max_iter=2000,\n",
    "        thr=1e-5,\n",
    "        alpha=alpha,\n",
    "        seed=123,\n",
    "        batch_size=batch_size,\n",
    "        verbose=False,\n",
    "        optim=\"adam\"\n",
    "    )\n",
    "\n",
    "    model.train(X, Y)\n",
    "\n",
    "    score = model.loss(Y,model.y_hat)  # need to do on val set\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "storage = \"sqlite:///optuna_mlpsoftmax111.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"MLP\", storage=storage, load_if_exists=True\n",
    ")  # 'minimize' for loss functions\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a3077f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best value': [1.0988307655332203],\n",
       " 'params': {'batch_size': 529,\n",
       "  'alpha': 0.013827387119549409,\n",
       "  'dropout': 0.7932257008509562}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results={\"best value\" : study.best_trial.values,\"params\": study.best_trial.params}\n",
    "best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d904ab",
   "metadata": {},
   "source": [
    "run model on optimised parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "662363f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "iteration 0 : TRAIN accuracy_score  : 0.5955555555555555, loss : 1.0132966656360094\n",
      "iteration 0 : TEST accuracy_score  : 0.5886666666666667, loss : 1.017541762998313\n",
      "-------------------------------------------------------------------------\n",
      "iteration 50 : TRAIN accuracy_score  : 0.748, loss : 0.6437553293458609\n",
      "iteration 50 : TEST accuracy_score  : 0.7426666666666667, loss : 0.6585968766631872\n",
      "early stopping at epoch 98\n",
      "final accuracy_score 0.7546666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = MLP_Classifier(\n",
    "    (\n",
    "        (\n",
    "            Layer(\n",
    "                nb_neurons=20,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                batchnorm=True\n",
    "         \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=10,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=30,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"dropout\", best_results[\"params\"][\"dropout\"]),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    max_iter=2000,\n",
    "    thr=1e-5,\n",
    "    alpha=best_results[\"params\"][\"alpha\"],\n",
    "    seed=123,\n",
    "    batch_size=best_results[\"params\"][\"batch_size\"],\n",
    "    verbose=True,\n",
    "    optim=\"adam\",\n",
    "    nb_epochs_early_stopping=50\n",
    ")\n",
    "\n",
    "fct=accuracy_score\n",
    "\n",
    "model.train(X_train, y_train,X_test,y_test,fct)\n",
    "\n",
    "print(f\"final {fct.__name__}\", accuracy_score(model.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090ffd0",
   "metadata": {},
   "source": [
    "## CNN or Early AlexNet (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e7008",
   "metadata": {},
   "source": [
    "even if in this section we will not perform full alexnet infrastructure which is to heavy to compute, we will use basic CNN to perform calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dcee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ConvLayer,MaxPoolLayer,Layer,FlatLayer\n",
    "from cnn import CNN\n",
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51f3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't forget to normalise input data and think about Batch normalisations\n"
     ]
    }
   ],
   "source": [
    "#define CNN infra, note that for CNN i use vanilla SGD only for simplicity reasons\n",
    "#moreover, i dont have any batchnormalisation on cnn part\n",
    "#mlp part will still have batch normalisation as before \n",
    "#i always suppose here that i have a clear alternation : Conv-> Act->Maxpool->Conv-> Act->Maxpool->Conv-> Act->Maxpool->Conv-> Act->Maxpool-> Flatten->MLP\n",
    "q=CNN(\n",
    "\n",
    "    (\n",
    "        ConvLayer(in_channels=3,output_channels=16,kernel_size=3,stride=1,padding=True,activation_function=\"relu\",initial=\"lecun\",law=\"normal\"),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=False),\n",
    "        ConvLayer(in_channels=16,output_channels=32,kernel_size=3,stride=1,padding=True,activation_function=\"relu\",initial=\"lecun\",law=\"normal\"),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=False),\n",
    "        ConvLayer(in_channels=32,output_channels=64,kernel_size=3,stride=1,padding=True,activation_function=\"relu\",initial=\"lecun\",law=\"normal\"),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=False),\n",
    "        \n",
    "        FlatLayer(),\n",
    "        Layer(\n",
    "                nb_neurons=64,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.001),\n",
    "                initial=\"he\",\n",
    "                batchnorm=True\n",
    "         \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=32,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.001),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "    \n",
    "        \n",
    "    ),\n",
    "    max_iter=3,#i will stop at 3 otherwise it will make too long to run \n",
    "    thr=1e-5,\n",
    "    alpha=0.001,\n",
    "    seed=123,\n",
    "    batch_size=100,\n",
    "    verbose=True,\n",
    "    nb_epochs_early_stopping=20\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c33e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 20:35:13.648143: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-05 20:35:13.649022: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 20:35:13.768242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 20:35:15.992615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 20:35:15.993979: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/opt/python/lib/python3.13/site-packages/keras/src/datasets/cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: (1000, 32, 32, 3) (1000, 1)\n",
      "FINAL SHAPES\n",
      "X_train: (32, 32, 3, 800)\n",
      "Y_train: (800, 1)\n",
      "X_test : (32, 32, 3, 200)\n",
      "Y_test : (200, 1)\n",
      "Train balance: [400 400]\n",
      "Test balance : [100 100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "\n",
    "# LOAD CIFAR10\n",
    "(X_train_raw, Y_train_raw), (X_test_raw, Y_test_raw) = cifar10.load_data()\n",
    "\n",
    "X = np.concatenate([X_train_raw, X_test_raw], axis=0)   # (60000,32,32,3)\n",
    "Y = np.concatenate([Y_train_raw, Y_test_raw], axis=0)   # (60000,1)\n",
    "\n",
    "#SELECT 2 CLASSES (0 airplane, 1 automobile) ( my class supports multiclass classification but i select binary classification as otherwise the results will be bad for only 1K images)\n",
    "mask = np.isin(Y.flatten(), [0,1])\n",
    "X = X[mask]\n",
    "Y = Y[mask]\n",
    "Y = (Y == 1).astype(int)\n",
    "#select only 1K observations : otherwise it is too heavy for local computations\n",
    "N = 1000\n",
    "pos = np.where(Y.flatten() == 1)[0]\n",
    "neg = np.where(Y.flatten() == 0)[0]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pos)\n",
    "np.random.shuffle(neg)\n",
    "k = N // 2\n",
    "selected_idx = np.concatenate([pos[:k], neg[:k]])\n",
    "np.random.shuffle(selected_idx)\n",
    "X = X[selected_idx]\n",
    "Y = Y[selected_idx]\n",
    "print(\"Using dataset:\", X.shape, Y.shape)\n",
    "#normalise data\n",
    "X = X.astype(np.float32) / 255\n",
    "\n",
    "#train test split \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    stratify=Y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "#CONVERT TO (H,W,C,N)\n",
    "\n",
    "X_train = np.transpose(X_train, (1,2,3,0))   # (32,32,3,N_train)\n",
    "X_test  = np.transpose(X_test,  (1,2,3,0))   # (32,32,3,N_test)\n",
    "\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "Y_test  = Y_test.reshape(-1,1)\n",
    "\n",
    "print(\"FINAL SHAPES\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"Y_test :\", Y_test.shape)\n",
    "print(\"Train balance:\", np.bincount(Y_train.flatten()))\n",
    "print(\"Test balance :\", np.bincount(Y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d964b02",
   "metadata": {},
   "source": [
    "we will run on train and evaluate on test datasets, since our architecture is weak (not Alexnet),few neurons and output channels, only 1K observations and 2 labels we will have bad results for accuracy\n",
    "in every case it is not very important as depends on our computers, the most important is the theoretical results, derivations and OOP applied in this project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea1012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3, 1)\n",
      "0 original image  shape: (32, 32, 3, 1)\n",
      "1 conv layer shape: (32, 32, 16, 1)\n",
      "2 maxpool layer shape: (15, 15, 16, 1)\n",
      "3 conv layer shape: (15, 15, 32, 1)\n",
      "4 maxpool layer shape: (7, 7, 32, 1)\n",
      "5 conv layer shape: (7, 7, 64, 1)\n",
      "6 maxpool layer shape: (3, 3, 64, 1)\n",
      "7 flatten layer shape: (3, 3, 64, 1)\n",
      "dummy res shape (1, 576)\n",
      "the last dim is 1 as it is dummy, other shapes are kept as is \n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "-------------------------------------------------------------------------\n",
      "iteration 0 : TRAIN accuracy_score  : 0.46875, loss : 0.6958009567468113\n",
      "iteration 0 : TEST accuracy_score  : 0.51, loss : 0.693897294925197\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "-------------------------------------------------------------------------\n",
      "iteration 1 : TRAIN accuracy_score  : 0.46875, loss : 0.69552350805603\n",
      "iteration 1 : TEST accuracy_score  : 0.525, loss : 0.6936780907733616\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "(100, 576)\n",
      "7 FlatLayer\n",
      "6 MaxPoolLayer\n",
      "5 ConvLayer\n",
      "4 MaxPoolLayer\n",
      "3 ConvLayer\n",
      "2 MaxPoolLayer\n",
      "1 ConvLayer\n",
      "-------------------------------------------------------------------------\n",
      "iteration 2 : TRAIN accuracy_score  : 0.47625, loss : 0.6952539830154053\n",
      "iteration 2 : TEST accuracy_score  : 0.54, loss : 0.6934660522458108\n",
      "Model terminated successfully, Did not Converge at 3 epoch, for a given alpha :  0.001 and given threshold : 1e-05 \n"
     ]
    }
   ],
   "source": [
    "q.train(\n",
    "     \n",
    "        X_train,\n",
    "        Y_train,\n",
    "        X_test ,\n",
    "        Y_test ,\n",
    "        fct  = accuracy_score\n",
    "\n",
    "\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
