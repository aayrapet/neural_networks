{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a1acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import MLP_Classifier,Layer,accuracy\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67427120",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Generate  dataset\n",
    "X, Y = make_classification(\n",
    "    n_samples=1000,     \n",
    "    n_features=4,       \n",
    "    n_redundant=0,      \n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,         # Add label noise\n",
    "    class_sep=1.0,      # Class separation\n",
    "    n_classes=2,      # nb classes\n",
    ")\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 0.5, X.shape)\n",
    "X = X + noise\n",
    "X=pd.DataFrame(X)\n",
    "Y=pd.Series(Y)\n",
    "if len(np.unique(Y))==2:\n",
    "   Y=pd.DataFrame(Y)\n",
    "else: \n",
    "   Y=pd.get_dummies(Y).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8c3c0",
   "metadata": {},
   "source": [
    "determine objective function to optimise :  minimise cross entropy (in pdf maximise log-likelihood).\n",
    "\n",
    "for example we optimise over batch size, learning rate and dropout (one of the most important parameters in NN).\n",
    "\n",
    "we could do also on layers but computantionally expensive for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15db729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:52:32,157] Using an existing study with name 'MLP' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't forget to normalise input data and think about Batch normalisations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:52:33,030] Trial 41 finished with value: 0.45006786377067426 and parameters: {'batch_size': 672, 'alpha': 0.06796381033487839, 'dropout': 0.609613634717613}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:35,194] Trial 42 finished with value: 0.4080460878352336 and parameters: {'batch_size': 734, 'alpha': 0.0623023541304181, 'dropout': 0.6416628842394265}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:36,060] Trial 43 finished with value: 0.4359845939288715 and parameters: {'batch_size': 698, 'alpha': 0.07823572663930908, 'dropout': 0.6209358023560235}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:36,565] Trial 44 finished with value: 0.5569548459952196 and parameters: {'batch_size': 705, 'alpha': 0.07240904831743597, 'dropout': 0.5479516377178313}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:38,919] Trial 45 finished with value: 0.4138217694811252 and parameters: {'batch_size': 685, 'alpha': 0.05919963547067779, 'dropout': 0.6646721456540988}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:39,784] Trial 46 finished with value: 0.4555222127473935 and parameters: {'batch_size': 648, 'alpha': 0.05384502889870325, 'dropout': 0.5906252131399047}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:41,701] Trial 47 finished with value: 0.4173245203936899 and parameters: {'batch_size': 545, 'alpha': 0.04760211802094544, 'dropout': 0.7007214440222828}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:44,675] Trial 48 finished with value: 0.40092186114495454 and parameters: {'batch_size': 564, 'alpha': 0.06843693038873762, 'dropout': 0.6354561689449733}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:51,103] Trial 49 finished with value: 0.4160311490385652 and parameters: {'batch_size': 729, 'alpha': 0.08008405535854189, 'dropout': 0.5334958446890563}. Best is trial 10 with value: 0.3929574144038384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model terminated successfully, Did not Converge at 2000 epoch, for a given alpha :  0.08008405535854189 and given threshold : 1e-05 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:52:53,255] Trial 50 finished with value: 0.397477263731721 and parameters: {'batch_size': 629, 'alpha': 0.08582385357178272, 'dropout': 0.7250688402963831}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:52:59,511] Trial 51 finished with value: 0.39415901695101807 and parameters: {'batch_size': 542, 'alpha': 0.09647757228978489, 'dropout': 0.6880826095775431}. Best is trial 10 with value: 0.3929574144038384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model terminated successfully, Did not Converge at 2000 epoch, for a given alpha :  0.09647757228978489 and given threshold : 1e-05 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:53:00,679] Trial 52 finished with value: 0.4180221083008415 and parameters: {'batch_size': 538, 'alpha': 0.0942336083448913, 'dropout': 0.6768793774907153}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:05,032] Trial 53 finished with value: 0.39401136289160776 and parameters: {'batch_size': 603, 'alpha': 0.0989461452425066, 'dropout': 0.691045506455889}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:07,876] Trial 54 finished with value: 0.4035956068692178 and parameters: {'batch_size': 598, 'alpha': 0.09070422534608173, 'dropout': 0.6900116271045297}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:08,847] Trial 55 finished with value: 0.42547475369801746 and parameters: {'batch_size': 590, 'alpha': 0.09554375730668144, 'dropout': 0.7307076331502801}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:09,800] Trial 56 finished with value: 0.4610732849145131 and parameters: {'batch_size': 515, 'alpha': 0.0416332796101051, 'dropout': 0.7655458619542587}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:14,936] Trial 57 finished with value: 0.3977832711360472 and parameters: {'batch_size': 718, 'alpha': 0.08484551541416105, 'dropout': 0.7081931224001916}. Best is trial 10 with value: 0.3929574144038384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model terminated successfully, Did not Converge at 2000 epoch, for a given alpha :  0.08484551541416105 and given threshold : 1e-05 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:53:19,031] Trial 58 finished with value: 0.39810274182747163 and parameters: {'batch_size': 615, 'alpha': 0.0740842105204432, 'dropout': 0.6632934708219554}. Best is trial 10 with value: 0.3929574144038384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model terminated successfully, Did not Converge at 2000 epoch, for a given alpha :  0.0740842105204432 and given threshold : 1e-05 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 10:53:20,080] Trial 59 finished with value: 0.4033986989928147 and parameters: {'batch_size': 680, 'alpha': 0.08941156114757785, 'dropout': 0.8074366551393992}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 10:53:21,346] Trial 60 finished with value: 0.40721674167006777 and parameters: {'batch_size': 779, 'alpha': 0.09977973848614705, 'dropout': 0.688125660776427}. Best is trial 10 with value: 0.3929574144038384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': 714, 'alpha': 0.04353166651137545, 'dropout': 0.8963721690959264}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 500, 800)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.1)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.5, 0.9)\n",
    "\n",
    "    model = MLP_Classifier(\n",
    "        (\n",
    "            [\n",
    "                Layer(\n",
    "                    nb_neurons=20,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=10,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=30,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"dropout\", dropout_rate),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        max_iter=2000,\n",
    "        thr=1e-5,\n",
    "        alpha=alpha,\n",
    "        seed=123,\n",
    "        batch_size=batch_size,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    model.train(X, Y)\n",
    "\n",
    "    score = model.loss(Y)  # need to do on val set\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "storage = \"sqlite:///optuna_mlp.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"MLP\", storage=storage, load_if_exists=True\n",
    ")  # 'minimize' for loss functions\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3077f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best value': [0.3929574144038384],\n",
       " 'params': {'batch_size': 714,\n",
       "  'alpha': 0.04353166651137545,\n",
       "  'dropout': 0.8963721690959264}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results={\"best value\" : study.best_trial.values,\"params\": study.best_trial.params}\n",
    "best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d904ab",
   "metadata": {},
   "source": [
    "run model on optimised parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662363f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 : accuracy  : 0.636, loss : 0.6912064731705144\n",
      "iteration 100 : accuracy  : 0.794, loss : 0.5593979229332793\n",
      "iteration 200 : accuracy  : 0.83, loss : 0.4590097088035097\n",
      "iteration 300 : accuracy  : 0.829, loss : 0.43923466186811005\n",
      "iteration 400 : accuracy  : 0.831, loss : 0.427387640599838\n",
      "iteration 500 : accuracy  : 0.83, loss : 0.4207655838681293\n",
      "iteration 600 : accuracy  : 0.829, loss : 0.4107991680521842\n",
      "iteration 700 : accuracy  : 0.84, loss : 0.4031294167460206\n",
      "iteration 800 : accuracy  : 0.835, loss : 0.4069390439872127\n",
      "iteration 900 : accuracy  : 0.839, loss : 0.39642508923937514\n",
      "iteration 1000 : accuracy  : 0.823, loss : 0.39801424641481165\n",
      "iteration 1100 : accuracy  : 0.83, loss : 0.3955574568176502\n",
      "iteration 1200 : accuracy  : 0.837, loss : 0.39397585667442775\n",
      "Model terminated successfully, Converged at 1288 epoch, for a given alpha :  0.04353166651137545 and given threshold : 1e-05 \n",
      "final accuracy 0.841\n"
     ]
    }
   ],
   "source": [
    "model = MLP_Classifier(\n",
    "    (\n",
    "        [\n",
    "            Layer(\n",
    "                nb_neurons=20,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=10,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=30,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"dropout\", best_results[\"params\"][\"dropout\"]),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    max_iter=2000,\n",
    "    thr=1e-5,\n",
    "    alpha=best_results[\"params\"][\"alpha\"],\n",
    "    seed=123,\n",
    "    batch_size=best_results[\"params\"][\"batch_size\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model.train(X, Y)\n",
    "\n",
    "print(\"final accuracy\", accuracy(model.predict(X), np.array(Y)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
