{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a1acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import MLP_Classifier,Layer\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a00a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67427120",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Generate  dataset\n",
    "X, Y = make_classification(\n",
    "    n_samples=1000,     \n",
    "    n_features=4,       \n",
    "    n_redundant=0,      \n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,         # Add label noise\n",
    "    class_sep=1.0,      # Class separation\n",
    "    n_classes=2,      # nb classes\n",
    ")\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 0.5, X.shape)\n",
    "X = X + noise\n",
    "X=pd.DataFrame(X)\n",
    "Y=pd.Series(Y)\n",
    "if len(np.unique(Y))==2:\n",
    "   Y=pd.DataFrame(Y)\n",
    "else: \n",
    "   Y=pd.get_dummies(Y).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8c3c0",
   "metadata": {},
   "source": [
    "determine objective function to optimise :  minimise cross entropy (in pdf maximise log-likelihood).\n",
    "\n",
    "for example we optimise over batch size, learning rate and dropout (one of the most important parameters in NN).\n",
    "\n",
    "we could do also on layers but computantionally expensive for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15db729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 16:20:22,874] Using an existing study with name 'MLP' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't forget to normalise input data and think about Batch normalisations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 16:20:30,253] Trial 14 finished with value: 0.3893226337249294 and parameters: {'batch_size': 750, 'alpha': 0.03872595936458154, 'dropout': 0.51857720234828}. Best is trial 1 with value: 0.3831267767085844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model terminated successfully, Did not Converge at 2000 epoch, for a given alpha :  0.03872595936458154 and given threshold : 1e-05 \n",
      "Best Hyperparameters: {'batch_size': 566, 'alpha': 0.07733463758642894, 'dropout': 0.8006816804579144}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 500, 800)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.1)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.5, 0.9)\n",
    "\n",
    "    model = MLP_Classifier(\n",
    "        (\n",
    "            (\n",
    "                Layer(\n",
    "                    nb_neurons=20,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                    batchnorm=True\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=10,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=30,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"dropout\", dropout_rate),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "        max_iter=2000,\n",
    "        thr=1e-5,\n",
    "        alpha=alpha,\n",
    "        seed=123,\n",
    "        batch_size=batch_size,\n",
    "        verbose=False,\n",
    "        optim=\"adam\"\n",
    "    )\n",
    "\n",
    "    model.train(X, Y)\n",
    "\n",
    "    score = model.loss(Y,model.y_hat)  # need to do on val set\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "storage = \"sqlite:///optuna_mlpsoftmax.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"MLP\", storage=storage, load_if_exists=True\n",
    ")  # 'minimize' for loss functions\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3077f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best value': [0.3831267767085844],\n",
       " 'params': {'batch_size': 566,\n",
       "  'alpha': 0.07733463758642894,\n",
       "  'dropout': 0.8006816804579144}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results={\"best value\" : study.best_trial.values,\"params\": study.best_trial.params}\n",
    "best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d904ab",
   "metadata": {},
   "source": [
    "run model on optimised parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662363f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "iteration 0 : TRAIN accuracy_score  : 0.764, loss : 0.5194497385823942\n",
      "iteration 0 : TEST accuracy_score  : 0.8, loss : 0.5012608734679342\n",
      "-------------------------------------------------------------------------\n",
      "iteration 50 : TRAIN accuracy_score  : 0.82, loss : 0.40533498454603833\n",
      "iteration 50 : TEST accuracy_score  : 0.828, loss : 0.3876527820670457\n",
      "-------------------------------------------------------------------------\n",
      "iteration 100 : TRAIN accuracy_score  : 0.8333333333333334, loss : 0.393268697951974\n",
      "iteration 100 : TEST accuracy_score  : 0.852, loss : 0.3699709167011068\n",
      "early stopping at epoch 114\n",
      "final accuracy_score 0.836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = MLP_Classifier(\n",
    "    (\n",
    "        (\n",
    "            Layer(\n",
    "                nb_neurons=20,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                batchnorm=True\n",
    "         \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=10,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=30,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"dropout\", best_results[\"params\"][\"dropout\"]),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    max_iter=2000,\n",
    "    thr=1e-5,\n",
    "    alpha=best_results[\"params\"][\"alpha\"],\n",
    "    seed=123,\n",
    "    batch_size=best_results[\"params\"][\"batch_size\"],\n",
    "    verbose=True,\n",
    "    optim=\"adam\",\n",
    "    nb_epochs_early_stopping=50\n",
    ")\n",
    "\n",
    "fct=accuracy_score\n",
    "\n",
    "model.train(X_train, y_train,X_test,y_test,fct)\n",
    "\n",
    "print(f\"final {fct.__name__}\", accuracy_score(model.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dcee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ConvLayer,MaxPoolLayer,Layer,FlatLayer\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51f3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't forget to normalise input data and think about Batch normalisations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q=CNN(\n",
    "\n",
    "    (\n",
    "        ConvLayer(in_channels=3,output_channels=16,kernel_size=2,stride=1,padding=True,activation_function=\"relu\",regul=None,initial=\"lecun\",law=\"normal\",batchnorm=False),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=True),\n",
    "        ConvLayer(in_channels=16,output_channels=32,kernel_size=2,stride=1,padding=True,activation_function=\"relu\",regul=None,initial=\"lecun\",law=\"normal\",batchnorm=False),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=True),\n",
    "        ConvLayer(in_channels=32,output_channels=64,kernel_size=2,stride=1,padding=True,activation_function=\"relu\",regul=None,initial=\"lecun\",law=\"normal\",batchnorm=False),\n",
    "        MaxPoolLayer(kernel_size=3,stride=2,padding=True),\n",
    "        FlatLayer(),\n",
    "        Layer(\n",
    "                nb_neurons=20,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                batchnorm=True\n",
    "         \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=10,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=30,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"dropout\",0.5),\n",
    "                initial=\"he\",\n",
    "                \n",
    "            ),\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40960877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'layer_type': layers.ConvLayer,\n",
       "  'in_channels': 3,\n",
       "  'output_channels': 16,\n",
       "  'kernel_size': 2,\n",
       "  'stride': 1,\n",
       "  'padding': True,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': None,\n",
       "  'regul_param': None,\n",
       "  'init': 'lecun',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': False,\n",
       "  'fct': <function torch_cnn_operations.convolution_torch(img_array_perm, kernels_perm, bias, padding=True, stride=1)>},\n",
       " 2: {'layer_type': layers.MaxPoolLayer,\n",
       "  'kernel_size': 3,\n",
       "  'stride': 2,\n",
       "  'padding': True,\n",
       "  'fct': <function torch_cnn_operations.maxpooling_torch(img_array_perm, kernel_size, stride=2, padding=True)>},\n",
       " 3: {'layer_type': layers.ConvLayer,\n",
       "  'in_channels': 16,\n",
       "  'output_channels': 32,\n",
       "  'kernel_size': 2,\n",
       "  'stride': 1,\n",
       "  'padding': True,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': None,\n",
       "  'regul_param': None,\n",
       "  'init': 'lecun',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': False,\n",
       "  'fct': <function torch_cnn_operations.convolution_torch(img_array_perm, kernels_perm, bias, padding=True, stride=1)>},\n",
       " 4: {'layer_type': layers.MaxPoolLayer,\n",
       "  'kernel_size': 3,\n",
       "  'stride': 2,\n",
       "  'padding': True,\n",
       "  'fct': <function torch_cnn_operations.maxpooling_torch(img_array_perm, kernel_size, stride=2, padding=True)>},\n",
       " 5: {'layer_type': layers.ConvLayer,\n",
       "  'in_channels': 32,\n",
       "  'output_channels': 64,\n",
       "  'kernel_size': 2,\n",
       "  'stride': 1,\n",
       "  'padding': True,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': None,\n",
       "  'regul_param': None,\n",
       "  'init': 'lecun',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': False,\n",
       "  'fct': <function torch_cnn_operations.convolution_torch(img_array_perm, kernels_perm, bias, padding=True, stride=1)>},\n",
       " 6: {'layer_type': layers.MaxPoolLayer,\n",
       "  'kernel_size': 3,\n",
       "  'stride': 2,\n",
       "  'padding': True,\n",
       "  'fct': <function torch_cnn_operations.maxpooling_torch(img_array_perm, kernel_size, stride=2, padding=True)>},\n",
       " 7: {'layer_type': layers.FlatLayer,\n",
       "  'fct': <function torch_cnn_operations.flatten_torch(img_array_perm)>},\n",
       " 8: {'layer_type': layers.Layer,\n",
       "  'nb_neurons': 20,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': 'l2',\n",
       "  'regul_param': 0.1,\n",
       "  'init': 'he',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': True},\n",
       " 9: {'layer_type': layers.Layer,\n",
       "  'nb_neurons': 10,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': 'l2',\n",
       "  'regul_param': 0.1,\n",
       "  'init': 'he',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': False},\n",
       " 10: {'layer_type': layers.Layer,\n",
       "  'nb_neurons': 30,\n",
       "  'activ_fct': 'relu',\n",
       "  'regul': 'dropout',\n",
       "  'regul_param': 0.5,\n",
       "  'init': 'he',\n",
       "  'law': 'normal',\n",
       "  'batchnorm': False}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb8acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.nb_cnn_layers+q.nb_layers#total\n",
    "\n",
    "from q.nb_cnn_layers+1 to total+1 do gradients mlp \n",
    "\n",
    "\n",
    "from 1 to q.nb_cnn_layers+1 do gradients cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f804a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b558d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 3, 3])\n",
      "(27, 144)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3)\n",
    "print(conv.weight.shape)\n",
    "torch.Size([1, 1, 2, 2])\n",
    "\n",
    "print(nn.init._calculate_fan_in_and_fan_out(conv.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a998e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 144)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanin=3*3*3\n",
    "FANOUT=3*3*16\n",
    "fanin,FANOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ede39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
