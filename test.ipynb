{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a1acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import MLP_Classifier,Layer,accuracy\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67427120",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Generate  dataset\n",
    "X, Y = make_classification(\n",
    "    n_samples=1000,     \n",
    "    n_features=4,       \n",
    "    n_redundant=0,      \n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,         # Add label noise\n",
    "    class_sep=1.0,      # Class separation\n",
    "    n_classes=2,      # nb classes\n",
    ")\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 0.5, X.shape)\n",
    "X = X + noise\n",
    "X=pd.DataFrame(X)\n",
    "Y=pd.Series(Y)\n",
    "if len(np.unique(Y))==2:\n",
    "   Y=pd.DataFrame(Y)\n",
    "else: \n",
    "   Y=pd.get_dummies(Y).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8c3c0",
   "metadata": {},
   "source": [
    "determine objective function to optimise :  minimise cross entropy (in pdf maximise log-likelihood).\n",
    "\n",
    "for example we optimise over batch size, learning rate and dropout (one of the most important parameters in NN).\n",
    "\n",
    "we could do also on layers but computantionally expensive for large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15db729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-06 14:10:30,929] Using an existing study with name 'MLP' instead of creating a new one.\n",
      "[I 2025-10-06 14:10:33,739] Trial 68 finished with value: 0.39635769032420504 and parameters: {'batch_size': 660, 'alpha': 0.05305922404049647, 'dropout': 0.6701779055276591}. Best is trial 10 with value: 0.3929574144038384.\n",
      "[I 2025-10-06 14:10:34,518] Trial 69 finished with value: 0.3902171042180091 and parameters: {'batch_size': 534, 'alpha': 0.07687725243652464, 'dropout': 0.7341042753305932}. Best is trial 69 with value: 0.3902171042180091.\n",
      "[I 2025-10-06 14:10:38,217] Trial 70 finished with value: 0.3840466996668747 and parameters: {'batch_size': 534, 'alpha': 0.07512249196749742, 'dropout': 0.7355842477137954}. Best is trial 70 with value: 0.3840466996668747.\n",
      "[I 2025-10-06 14:10:42,791] Trial 71 finished with value: 0.37838720432456874 and parameters: {'batch_size': 517, 'alpha': 0.08383707724593538, 'dropout': 0.8770937426215277}. Best is trial 71 with value: 0.37838720432456874.\n",
      "[I 2025-10-06 14:10:43,676] Trial 72 finished with value: 0.38547273382660147 and parameters: {'batch_size': 518, 'alpha': 0.0759521994930465, 'dropout': 0.8758079608249316}. Best is trial 71 with value: 0.37838720432456874.\n",
      "[I 2025-10-06 14:10:44,918] Trial 73 finished with value: 0.37909260645198417 and parameters: {'batch_size': 522, 'alpha': 0.07790609436869148, 'dropout': 0.8683735777905268}. Best is trial 71 with value: 0.37838720432456874.\n",
      "[I 2025-10-06 14:10:45,895] Trial 74 finished with value: 0.37534649553560645 and parameters: {'batch_size': 534, 'alpha': 0.08408839297279899, 'dropout': 0.8714076961172885}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:46,275] Trial 75 finished with value: 0.3940687199454445 and parameters: {'batch_size': 517, 'alpha': 0.07579257479858369, 'dropout': 0.8794732332293401}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:47,961] Trial 76 finished with value: 0.383707100428581 and parameters: {'batch_size': 534, 'alpha': 0.08327757091289185, 'dropout': 0.854753316707681}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:49,924] Trial 77 finished with value: 0.38040335066103487 and parameters: {'batch_size': 536, 'alpha': 0.08342949994382935, 'dropout': 0.8516164648871127}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:51,095] Trial 78 finished with value: 0.3990589911554777 and parameters: {'batch_size': 535, 'alpha': 0.07843994887942488, 'dropout': 0.8536914721460092}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:54,140] Trial 79 finished with value: 0.38309071408716283 and parameters: {'batch_size': 517, 'alpha': 0.08319886557267114, 'dropout': 0.8450210157879147}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:10:56,837] Trial 80 finished with value: 0.38003605713093863 and parameters: {'batch_size': 519, 'alpha': 0.08417895224795366, 'dropout': 0.850092181484962}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:01,806] Trial 81 finished with value: 0.38960242788643573 and parameters: {'batch_size': 505, 'alpha': 0.08404103645037901, 'dropout': 0.8461842287465142}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:02,982] Trial 82 finished with value: 0.3806491867964535 and parameters: {'batch_size': 520, 'alpha': 0.08261371290460873, 'dropout': 0.8646580190187337}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:03,315] Trial 83 finished with value: 0.38678849014471456 and parameters: {'batch_size': 526, 'alpha': 0.08203864202073771, 'dropout': 0.8613068360590229}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:05,899] Trial 84 finished with value: 0.3843141049221922 and parameters: {'batch_size': 511, 'alpha': 0.0875261294109544, 'dropout': 0.8301152980162521}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:10,221] Trial 85 finished with value: 0.3807149433765423 and parameters: {'batch_size': 526, 'alpha': 0.08376345363271828, 'dropout': 0.8996854747048834}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:12,673] Trial 86 finished with value: 0.3821776849444508 and parameters: {'batch_size': 525, 'alpha': 0.08027019701410364, 'dropout': 0.8903553041875967}. Best is trial 74 with value: 0.37534649553560645.\n",
      "[I 2025-10-06 14:11:14,211] Trial 87 finished with value: 0.380937520537274 and parameters: {'batch_size': 521, 'alpha': 0.07932097076211758, 'dropout': 0.8980440214901153}. Best is trial 74 with value: 0.37534649553560645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': 534, 'alpha': 0.08408839297279899, 'dropout': 0.8714076961172885}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 500, 800)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.1)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.5, 0.9)\n",
    "\n",
    "    model = MLP_Classifier(\n",
    "        (\n",
    "            [\n",
    "                Layer(\n",
    "                    nb_neurons=20,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=10,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"l2\", 0.1),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "                Layer(\n",
    "                    nb_neurons=30,\n",
    "                    activation_function=\"relu\",\n",
    "                    regul=(\"dropout\", dropout_rate),\n",
    "                    initial=\"he\",\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        max_iter=2000,\n",
    "        thr=1e-5,\n",
    "        alpha=alpha,\n",
    "        seed=123,\n",
    "        batch_size=batch_size,\n",
    "        verbose=False,\n",
    "        optim=\"adam\"\n",
    "    )\n",
    "\n",
    "    model.train(X, Y)\n",
    "\n",
    "    score = model.loss(Y)  # need to do on val set\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "storage = \"sqlite:///optuna_mlp.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"MLP\", storage=storage, load_if_exists=True\n",
    ")  # 'minimize' for loss functions\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3077f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best value': [0.37534649553560645],\n",
       " 'params': {'batch_size': 534,\n",
       "  'alpha': 0.08408839297279899,\n",
       "  'dropout': 0.8714076961172885}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results={\"best value\" : study.best_trial.values,\"params\": study.best_trial.params}\n",
    "best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d904ab",
   "metadata": {},
   "source": [
    "run model on optimised parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662363f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 : accuracy  : 0.717, loss : 0.6047089945911341\n",
      "iteration 100 : accuracy  : 0.834, loss : 0.38891906801321363\n",
      "iteration 200 : accuracy  : 0.834, loss : 0.38933392266681666\n",
      "Model terminated successfully, Converged at 256 epoch, for a given alpha :  0.08408839297279899 and given threshold : 1e-05 \n",
      "final accuracy 0.845\n"
     ]
    }
   ],
   "source": [
    "model = MLP_Classifier(\n",
    "    (\n",
    "        [\n",
    "            Layer(\n",
    "                nb_neurons=20,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=10,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"l2\", 0.1),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "            Layer(\n",
    "                nb_neurons=30,\n",
    "                activation_function=\"relu\",\n",
    "                regul=(\"dropout\", best_results[\"params\"][\"dropout\"]),\n",
    "                initial=\"he\",\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    max_iter=2000,\n",
    "    thr=1e-5,\n",
    "    alpha=best_results[\"params\"][\"alpha\"],\n",
    "    seed=123,\n",
    "    batch_size=best_results[\"params\"][\"batch_size\"],\n",
    "    verbose=True,\n",
    "    optim=\"adam\"\n",
    ")\n",
    "\n",
    "model.train(X, Y)\n",
    "\n",
    "print(\"final accuracy\", accuracy(model.predict(X), np.array(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8946362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#future steps:\n",
    "\"\"\"\n",
    "-do dynamic learning rate\n",
    "-do split train test and val on test always in the training loop too\n",
    "-for a given set of hyperparameters (best) plot some losses for different opti methods \n",
    "-add differential evolution \n",
    "\n",
    "\n",
    "-add batch norm \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
