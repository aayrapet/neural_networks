# ğŸ§  NN-from-Scratch

*A journey into building a deep learning library from the ground up.*

---

## ğŸš€ Motivation

Instead of importing `sklearn.neural_network.MLPClassifier` or TensorFlowâ€™s `Dense` layer, this project is about **reinventing the wheel â€” on purpose**.  

The best way to truly understand:

- **Forward propagation**
- **Backpropagation**
- **Weight initialization**
- **Regularization**
- **Optimizers**

â€¦is to code them by hand.

This repo is my sandbox for building and experimenting with a **Multi-Layer Perceptron (MLP)** from scratch.

---

## ğŸ› ï¸ Features (so far)

- âœ… Custom `Layer` class to define architecture
- âœ… Strong parameter validation with friendly error messages
- âœ… Activation functions: **Sigmoid**, **ReLU**, **Tanh**
- âœ… Initializations: **Random**, **Xavier**, **He**, **LeCun**
- âœ… Additional optimizers: **SGD**, **Mini-batch GD**, **Adam**
- âœ… Loss functions: **Binary Cross-Entropy**, **Multi-class Cross-Entropy**
- âœ… Regularization: **L2**, **Dropout**
- âœ… Training loop with convergence check
- âœ… Debug prints for loss & accuracy during training

---

## ğŸ”® Roadmap (coming soon)

- â³ Batch Normalization
- â³ Visualization of training curves

---

## ğŸ“– Philosophy

This project isnâ€™t about outperforming PyTorch or TensorFlow.  
Itâ€™s about **education**: exposing the internals of an MLP and exploring each design choice along the way.  

Think of it as building your own "mini-Keras" to really **feel the math** behind deep learning.

---

## â–¶ï¸ Quick Example

```python
import pandas as pd
from main import Layer, MLP_Classifier

# Define architecture
layers = [
    Layer(8, "tanh", regul=("dropout", 0.5), initial="xavier"),
    Layer(4, "relu", regul=("l2", 0.9), initial="he"),
]

# Instantiate model
mlp = MLP_Classifier(layers, alpha=0.01, max_iter=1000, seed=42, optim="adam )

# Train (X, Y must be pandas DataFrames)
mlp.train(X_train, Y_train)

# Predict
preds = mlp.predict(X_test)

